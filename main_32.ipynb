{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement CNN for CIFAR-10 dataset using PyTorch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Transform to normalize the data\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]\n",
    "# )\n",
    "# Transform to normalize the data and for data augmentation\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 20\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data/\", train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data/\", train=False, transform=transform\n",
    ")\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network with following layers:\n",
    "# CONV1: with Kernel size (3 ×3), In channels 3, Out channels 32.\n",
    "# POOL1: Kernel size (2 ×2).\n",
    "# CONV2: Kernel size (5 ×5), In channels 32, Out channels 64\n",
    "# POOL2: Kernel size (2 ×2).\n",
    "# CONV3: Kernel size (3 ×3), In channels 64, Out channels 64.\n",
    "# FC1: Fully connected layer (also known as Linear layer) with 64 output neurons.\n",
    "# FC2: Fully connected layer with 10 output neurons.\n",
    "# Use ReLU as the activation function for all layers apart from the max-pooling layers, and the FC2layer. \n",
    "# You need to flatten the CONV3 output before passing as input to FC1. \n",
    "# and train for 20 epochs using an Adam optimizer with learning rate 0.001 and batch size 32. \n",
    "# Use the categorical cross entropy loss as the loss function\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # FC layers\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # Softmax\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "model = ConvNet(num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, define the loss function and optimizer. KLdivergence is used. \n",
    "criterion = nn.KLDivLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Set the learning rate scheduler\n",
    "# step_size = 5\n",
    "# gamma = 0.1\n",
    "# scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation loss and accuracy tracking\n",
    "epoch_train_loss = []\n",
    "epoch_train_acc = []\n",
    "epoch_val_loss = []\n",
    "epoch_val_acc = []\n",
    "\n",
    "# Now, train the model for 20 epochs\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    running_train_loss = 0\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Run the forward pass\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # # Update the learning rate using the scheduler\n",
    "        # scheduler.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "        # Track the accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = labels.size(0)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_train_loss.append(running_train_loss / len(train_loader))\n",
    "    epoch_train_acc.append(correct / float(total))\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    running_val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_val_loss.append(running_val_loss / len(test_loader))\n",
    "    epoch_val_acc.append(val_correct / float(val_total))\n",
    "\n",
    "    # Print the loss for every epoch\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss[-1]:.4f}, Train Acc: {epoch_train_acc[-1]:.4f},Val Loss: {epoch_val_loss[-1]:.4f}, Val Acc: {epoch_val_acc[-1]:.4f}\"\n",
    "    )\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), epoch_train_loss, label=\"Training Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), epoch_val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot the epoch vs. overall validation accuracy and training accuracy\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), epoch_val_acc, label=\"Validation Accuracy\")\n",
    "plt.plot(range(1, num_epochs + 1), epoch_train_acc, label=\"Training Accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Epoch vs. Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model for class-wise accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Class-wise accuracy\n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    for images, labels in test_loader:\n",
    "        # Run the forward pass\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # Get the predicted labels\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # Collect the correct predictions for each class\n",
    "        c = (predicted == labels)\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "    \n",
    "    # Print the accuracy for each class\n",
    "    for i in range(num_classes):\n",
    "        print(f'Accuracy of {classes[i]}: {100 * class_correct[i] / class_total[i]}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
